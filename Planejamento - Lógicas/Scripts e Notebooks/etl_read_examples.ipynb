{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Read and Manupulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL_READ_PONTOS_PRODUTOS\n",
    "\n",
    "_df = etl_pre_read_pontos_produtos\n",
    "\n",
    "#create column time\n",
    "_mes_dict = {1: '01', 2: '02', 3: '03', 4: '04', 5: '05', 6: '06', 7:'07', 8: '08', 9: '09', 10: '10', 11: '11', 12: '12'}\n",
    "_df['Mes'] = _df['Mes'].replace(_mes_dict)\n",
    "_df['Ano'] = _df['Ano'].astype(str)\n",
    "_df['time'] = _df['Ano'] + '.' + _df['Mes']\n",
    "_df = _df.drop(columns = ['Ano', 'Mes'])\n",
    "\n",
    "try:\n",
    "    _df['id_produto'] = _df['id_produto'].astype(int)\n",
    "except:\n",
    "    pass\n",
    "_df['id_produto'] = _df['id_produto'].astype(str)\n",
    "\n",
    "_df['pais_emitente'] = _df['pais_emitente'].str.upper()\n",
    "\n",
    "_df.columns = ['pais', 'cod_produto', 'pto_quali', 'pto_boni', 'time']\n",
    "\n",
    "\n",
    "_df = _df[_df['time']<current_month]\n",
    "\n",
    "\n",
    "result = _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_session\n",
    "from pyspark.sql import SparkSession\n",
    "if SparkSession._activeSession:\n",
    "    SparkSession._activeSession.stop()\n",
    "result = SparkSession.builder.master('local[*]') \\\n",
    "    .config(\"spark.local.dir\", \"/data/tmp\")\\\n",
    "    .appName(\"TestApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL_READ_SELLOUT_BU\n",
    "\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import col , udf\n",
    "\n",
    "_df = etl_read_sellout\n",
    "_df = _df.withColumnRenamed('cod_produto', 'id_produto')\n",
    "\n",
    "# ### reasign kits to product\n",
    "_df3 = read_kit_produto.copy()\n",
    "_df3 = _df3[['pais', 'cod_kit','cod_produto', 'quantidade']]#, 'descricao' ]]\n",
    "_df3 = _df3.rename(columns = {'cod_kit':'id_produto'})#, 'descricao': 'descricao_prod'})\n",
    "_df3 = spark_session.createDataFrame(_df3) \n",
    "_df = _df.join(_df3, on = (_df.pais== _df3.pais)&(_df.id_produto == _df3.id_produto), how = 'left').drop(_df3.pais).drop(_df3.id_produto)\n",
    "\n",
    "_df = _df.na.fill(0.0, 'receita_sellout_real')\n",
    "_df = _df.na.fill(0.0, 'Volume_sellout')\n",
    "_df = _df.na.fill(1, 'quantidade')\n",
    "\n",
    "_df = _df.withColumn('volume', sf.col('Volume_sellout')*sf.col('quantidade'))\n",
    "\n",
    "_df = _df.withColumn('cod_produto', sf.when(sf.col('cod_produto').isNull(), sf.col('id_produto')).otherwise(sf.col('cod_produto')))\n",
    "\n",
    "# _df['descr_produto'] = np.where(_df['descricao_prod'].notnull(), _df['descricao_prod'], _df['descr_produto'])\n",
    "\n",
    "#create column time\n",
    "_mes_dict = {1: '01', 2: '02', 3: '03', 4: '04', 5: '05', 6: '06', 7:'07', 8: '08', 9: '09', 10: '10', 11: '11', 12: '12'}\n",
    "\n",
    "map_func = udf(lambda row : _mes_dict.get(row,row))\n",
    "_df = _df.withColumn(\"Mes\", map_func(col(\"Mes\")))\n",
    "\n",
    "_df = _df.select(sf.concat_ws('.', _df.Ano, _df.Mes).alias('time'), sf.col(\"*\"))\n",
    "\n",
    "_df = _df.withColumn('volume',sf.col('volume').cast(\"float\").alias('volume'))\n",
    "_df = _df.withColumn('receita_sellout_real',sf.col('receita_sellout_real').cast(\"float\").alias('receita_sellout_real'))\n",
    "\n",
    "_df = _df.groupby('time', 'pais', 'cod_produto').agg(sf.sum(\"volume\").alias(\"volume\"),sf.sum(\"receita_sellout_real\").alias(\"receita\"))\n",
    "\n",
    "_df = _df.filter(_df.time<current_month)\n",
    "     \n",
    "result=_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datalake Service Client\n",
    "\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.identity import ClientSecretCredential     \n",
    "\n",
    "_account_url = 'https://datalake.dfs.core.windows.net'\n",
    "_client_id = '0c03'\n",
    "_tenant_id = '0a74a2'\n",
    "_client_secret = '9018Q'\n",
    "\n",
    "_credentialA = ClientSecretCredential(_tenant_id, _client_id, _client_secret)\n",
    "\n",
    "_service_client = DataLakeServiceClient(account_url=_account_url, credential=_credentialA)\n",
    "\n",
    "result = _service_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datalake File System Client\n",
    "\n",
    "result = etl_datalake_service_client.get_file_system_client(file_system='pyplan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read paths\n",
    "\n",
    "import os \n",
    "\n",
    "fileSystemClient=etl_datalake_file_system_client\n",
    "filesPath = os.path.join('consultores')\n",
    "# filesPath = None\n",
    "\n",
    "_f = fileSystemClient.get_paths(path=filesPath, recursive=True)\n",
    "\n",
    "for f in _f:\n",
    "    print(f)\n",
    "\n",
    "result = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Directory Example\n",
    "\n",
    "path_list = etl_datalake_file_system_client.get_paths( path= \"/\", recursive=False, max_results=500)\n",
    "\n",
    "res = []\n",
    "for path in path_list: \n",
    "    res.append(path.name)\n",
    "    print(path.name)\n",
    "    print(\"read_\" + path.name + '\\n')\n",
    "\n",
    "result = res[len(res)-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75b64ca35833826d255b5110be976b8b491e59919d991eceebad02336ca41c97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
