{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etl Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet from Part\n",
    "\n",
    "def _fn(\n",
    "    filePathName, \n",
    "    fileSystemClient,\n",
    "    dataTypes=None,\n",
    "    showMessages=True):\n",
    "    \n",
    "    import os\n",
    "    from io import BytesIO\n",
    "    \n",
    "    _path = os.path.split(filePathName)[0]\n",
    "    _file = os.path.split(filePathName)[1]\n",
    "    _file_client = fileSystemClient.get_directory_client(_path).get_file_client(_file)\n",
    "    _stream = BytesIO()\n",
    "    _file_data = _file_client.download_file().readinto(_stream)\n",
    "    _df = pd.read_parquet(_stream)\n",
    "    \n",
    "    if dataTypes is not None:\n",
    "        # Check if every col exists\n",
    "        if showMessages:\n",
    "            for col in dataTypes:\n",
    "                if col not in _df:\n",
    "                    pp.send_message(f\"Column '{col}' is not present in '{os.path.split(_path)[-1]}'\")\n",
    "        \n",
    "        # Must convert to float and then to pandas Int to support NaNs\n",
    "        _int_cols = {k:v for k,v in dataTypes.items() if v == 'int'}\n",
    "        # Category columns must be converted at the end (with all parts concatenated)\n",
    "        _other_cols = {k:v for k,v in dataTypes.items() if v not in ['int', 'category']}\n",
    "        _df = _df.astype(_other_cols)\n",
    "        for col in _int_cols:\n",
    "            _df[col] = _df[col].astype(float).astype('Int64')\n",
    "\n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe from Parquet Parts\n",
    "\n",
    "def _fn(\n",
    "    filesPath, \n",
    "    fileSystemClient,\n",
    "    dataTypes=None,\n",
    "    verbose=False,\n",
    "    showMessages=True):\n",
    "    \n",
    "    # Path generator\n",
    "    _path_list = fileSystemClient.get_paths(path=filesPath, recursive=True)\n",
    "\n",
    "    _dfs = []\n",
    "    for path in _path_list:\n",
    "        # Full path string\n",
    "        _path_name = path.name\n",
    "        if '.parquet' in _path_name:\n",
    "            try:\n",
    "                _file_df = read_parquet_from_part(\n",
    "                    filePathName=_path_name,\n",
    "                    fileSystemClient=fileSystemClient,\n",
    "                    dataTypes=dataTypes,\n",
    "                    showMessages=showMessages)\n",
    "                _dfs.append(_file_df)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(_path_name + '\\n')\n",
    "            except Exception as e:\n",
    "                if showMessages:\n",
    "                    pp.send_message(f\"There was an error reading '{_path_name}' file. Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    _df = pd.DataFrame(columns=list(dataTypes.keys()) if dataTypes is not None else [])\n",
    "    if len(_dfs) > 0:\n",
    "        _df = pd.concat(_dfs, ignore_index=True)\n",
    "    \n",
    "    if dataTypes is not None:\n",
    "        # Convert to categorical\n",
    "        _cat_cols = {k:v for k,v in dataTypes.items() if v == 'category'}\n",
    "        for col in _cat_cols:\n",
    "            _df[col] = _df[col].astype('category')\n",
    "    \n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV from Part\n",
    "def _fn(\n",
    "    filePathName, \n",
    "    fileSystemClient,\n",
    "    sep=',',\n",
    "    header='infer',\n",
    "    dataTypes=None,\n",
    "    skipBadLines=True, \n",
    "    names = None):\n",
    "    \n",
    "    import os\n",
    "    from io import StringIO\n",
    "    \n",
    "    _path = os.path.split(filePathName)[0]\n",
    "    _file = os.path.split(filePathName)[1]\n",
    "    _file_client = fileSystemClient.get_directory_client(_path).get_file_client(_file)\n",
    "    _file_data = StringIO(_file_client.download_file().readall().decode())\n",
    "    _error_bad_lines = not skipBadLines\n",
    "    _dtype = None\n",
    "    \n",
    "    if dataTypes is not None:\n",
    "        # Category columns must be converted at the end (with all parts concatenated)\n",
    "        _not_categ_cols_dtypes = {k:v for k,v in dataTypes.items() if v != 'category'}\n",
    "        _dtype = {k: ('Int64' if v == 'int' else v) for k,v in _not_categ_cols_dtypes.items()}\n",
    "    if names is None:\n",
    "        _df = pd.read_csv(\n",
    "            _file_data,\n",
    "            sep=sep,\n",
    "            header=header,\n",
    "            dtype=_dtype,\n",
    "            error_bad_lines=_error_bad_lines)\n",
    "    else:\n",
    "        _df = pd.read_csv(\n",
    "            _file_data,\n",
    "            sep=sep,\n",
    "            header=header,\n",
    "            dtype=_dtype,\n",
    "            error_bad_lines=_error_bad_lines,\n",
    "            names = names\n",
    "            )\n",
    "        \n",
    "    \n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe from CSV Parts\n",
    "\n",
    "def _fn(\n",
    "    filesPath, \n",
    "    fileSystemClient,\n",
    "    sep=',',\n",
    "    header='infer',\n",
    "    dataTypes=None,\n",
    "    skipBadLines=False,\n",
    "    verbose=False,\n",
    "    showMessages=True,\n",
    "    names = None\n",
    "    ):\n",
    "        \n",
    "    import os\n",
    "    \n",
    "    # Path generator\n",
    "    _path_list = fileSystemClient.get_paths(path=filesPath, recursive=True)\n",
    "\n",
    "    _dfs = []\n",
    "    for path in _path_list:\n",
    "        # Full path string\n",
    "        _path_name = path.name\n",
    "        if 'part' in _path_name and '.csv' in _path_name:\n",
    "            _file_df = read_csv_from_part(\n",
    "                filePathName=_path_name,\n",
    "                fileSystemClient=fileSystemClient,\n",
    "                sep=sep,\n",
    "                header=header,\n",
    "                dataTypes=dataTypes,\n",
    "                skipBadLines=skipBadLines,\n",
    "                names = names\n",
    "                )\n",
    "            _dfs.append(_file_df)\n",
    "            \n",
    "            if verbose:\n",
    "                print(_path_name + '\\n')\n",
    "    \n",
    "    _df = pd.DataFrame(columns=list(dataTypes.keys()) if dataTypes is not None else [])\n",
    "    if len(_dfs) > 0:\n",
    "        _df = pd.concat(_dfs, ignore_index=True)\n",
    "    \n",
    "    _flag = False\n",
    "    if dataTypes is not None:\n",
    "        # Check if every col exists\n",
    "        for col in dataTypes:\n",
    "            if col not in _df:\n",
    "                if showMessages:\n",
    "                    pp.send_message(f\"Column '{col}' is not present in '{os.path.split(filesPath)[-1]}'\")\n",
    "                _flag = True\n",
    "        \n",
    "        if _flag:\n",
    "            raise Exception(f\"Check columns from table '{os.path.split(filesPath)[-1]}'\")\n",
    "        \n",
    "        # Convert to categorical\n",
    "        _cat_cols = {k:v for k,v in dataTypes.items() if v == 'category'}\n",
    "        for col in _cat_cols:\n",
    "            _df[col] = _df[col].astype('category')\n",
    "    \n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Directory Files & Folders\n",
    "\n",
    "def _fn(\n",
    "    filesPath, \n",
    "    fileSystemClient, \n",
    "    maxResults=500):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Path generator\n",
    "    path_list = fileSystemClient.get_paths(path=filesPath, recursive=True, max_results=maxResults)\n",
    "\n",
    "    for path in path_list:\n",
    "        print(path.name + '\\n')\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Folder\n",
    "def _fn(path):\n",
    "    # Creates new folder if it does not exist\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pickle or Datalake Table\n",
    "\n",
    "def _fn(fileName, versionsPath, datalakeReadNode, readFromDatalakeByDefault=False):\n",
    "    \"\"\"datalakeReadNode must me a node object. i.e: etl_read_dm_material.node\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "\n",
    "    _path = os.path.join(versionsPath, fileName)\n",
    "    \n",
    "    if etl_datalake_tables_read_source_sel == 'Datalake':\n",
    "        _df = datalakeReadNode.result.copy()\n",
    "        datalakeReadNode.invalidate()\n",
    "    else:\n",
    "        if not readFromDatalakeByDefault:\n",
    "            if os.path.isfile(_path):\n",
    "                _df = pd.read_pickle(_path, compression=etl_pkl_compression_mode)\n",
    "            else:\n",
    "                raise ValueError(f\"File '{fileName}' does not exist\")\n",
    "        \n",
    "        elif versionsPath == etl_no_versions_msg or not os.path.isfile(_path):\n",
    "            _df = datalakeReadNode.result.copy()\n",
    "            datalakeReadNode.invalidate()\n",
    "            # Warn when reading from Datalake if the source should have been a saved version\n",
    "            pp.send_message(message_text=f\"Table '{fileName[:-4]}' was read directly from Datalake\")\n",
    "        \n",
    "        else:\n",
    "            _df = pd.read_pickle(_path, compression=etl_pkl_compression_mode)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Recurrent Tables are All Updated\n",
    "\n",
    "def _fn(recurrentTables, maxAttempts=3, showMessages=True):\n",
    "    import time as _time_mod\n",
    "    \n",
    "    if showMessages:\n",
    "        pp.progressbar(50, message_text='Checking recurrent tables last version')\n",
    "    \n",
    "    _n = 1\n",
    "    while True:\n",
    "        etl_read_controle_pyplan.node.invalidate()\n",
    "        _df = etl_read_controle_pyplan.copy()\n",
    "        \n",
    "        _versions = set()\n",
    "        for table in recurrentTables:\n",
    "            _df_table = _df[_df['NOME_BASE'] == table]\n",
    "            _last_update = _df_table['ATUALIZACAO'].max()\n",
    "            _last_version = _df_table[_df_table['ATUALIZACAO'] == _last_update]['VERSAO'].iloc[0]\n",
    "            _versions.add(_last_version)\n",
    "        \n",
    "        if len(_versions) != 1 and _n < maxAttempts:\n",
    "            _n += 1\n",
    "            _time_mod.sleep(5*60)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if showMessages:\n",
    "        pp.progressbar(100, message_text='Checking recurrent tables last version')\n",
    "    \n",
    "    return len(_versions) == 1\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Version of Datalake Tables\n",
    "\n",
    "def _fn(nodesList, targetPath, processName, maxAttempts=3, showMessages=True, checkRecurrentTables=False, recurrentTables=etl_recurrent_table_names, logsPath=etl_logs_file_path, nodesToSaveAfterList=None):\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    import datetime, pytz\n",
    "    import time as _time_mod\n",
    "    \n",
    "    _username = pyplan_user['userName']\n",
    "    _failed_table = ''\n",
    "    _logs = []\n",
    "    try:\n",
    "        _check = True\n",
    "        if checkRecurrentTables:\n",
    "            _check = check_if_recurrent_tables_are_all_updated(recurrentTables, maxAttempts=3, showMessages=showMessages)\n",
    "            \n",
    "        if _check:\n",
    "            _number_of_attempts = 3\n",
    "            \n",
    "            _version = datetime.datetime.now(pytz.timezone(etl_pytz_timezone)).strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            _temp_version_path = os.path.join(etl_temp_folder_path, _version)\n",
    "            _target_version_path = os.path.join(targetPath, _version)\n",
    "            \n",
    "            _start_time = _time_mod.time()\n",
    "            \n",
    "            # Create folders if they do not exist\n",
    "            create_folder(etl_root_path)\n",
    "            create_folder(etl_temp_folder_path)\n",
    "            create_folder(_temp_version_path)\n",
    "            create_folder(targetPath)\n",
    "            \n",
    "            _attempts = 1\n",
    "            while True:\n",
    "                _ok_status = True\n",
    "                _progress = 1\n",
    "                for node in nodesList:\n",
    "                    _tablename = node.identifier[9:].upper()  # removes etl_read_\n",
    "                    _filename = f'{_tablename}.pkl'\n",
    "                    _filepath = os.path.join(_temp_version_path, _filename)\n",
    "                    if showMessages:\n",
    "                        pp.progressbar(_progress, message_text=f'Attempt {_attempts}: Saving {_tablename}')\n",
    "                    try:\n",
    "                        # Save\n",
    "                        _result = node.result\n",
    "                        _result.to_pickle(_filepath, compression=etl_pkl_compression_mode)\n",
    "                        node.invalidate()  # release memory\n",
    "                        _status = 'OK'\n",
    "                        _message = ''\n",
    "                    except Exception as e:\n",
    "                        _status = 'ERROR'\n",
    "                        _message = e\n",
    "                        # Remove folder\n",
    "                        shutil.rmtree(_temp_version_path, ignore_errors=True)\n",
    "                        _ok_status = False\n",
    "                        _failed_table = _tablename\n",
    "                        break\n",
    "                    _log = create_log_row(processName, _username, _version, _tablename, _status, _message)\n",
    "                    _logs.append(_log)\n",
    "                    _progress = int(_progress + 1/len(nodesList)*100)\n",
    "                if _ok_status or _attempts == maxAttempts:\n",
    "                    break\n",
    "                else:\n",
    "                    _attempts += 1\n",
    "                    _time_mod.sleep(5*60)  # 5 minutes cooldown\n",
    "                    _log = create_log_row(processName, _username, _version, _failed_table, '', 'RETRYING')\n",
    "                    _logs.append(_log)\n",
    "            \n",
    "            # Transfer version to official versions folder\n",
    "            if _ok_status:\n",
    "                shutil.move(_temp_version_path, _target_version_path)\n",
    "                \n",
    "                # Save nodes that must me saved after the process\n",
    "                if nodesToSaveAfterList is not None:\n",
    "                    for node in nodesToSaveAfterList:\n",
    "                        _tablename = node.identifier[9:].upper()  # removes etl_read_\n",
    "                        _filename = f'{_tablename}.pkl'\n",
    "                        _filepath = os.path.join(_target_version_path, _filename)\n",
    "                        try:\n",
    "                            # Save\n",
    "                            _result = node.result\n",
    "                            _result.to_pickle(_filepath, compression=etl_pkl_compression_mode)\n",
    "                            node.invalidate()  # release memory\n",
    "                            _status = 'OK'\n",
    "                            _message = ''\n",
    "                        except Exception as e:\n",
    "                            _status = 'ERROR'\n",
    "                            _message = e\n",
    "                    _log = create_log_row(processName, _username, _version, _tablename, _status, _message)\n",
    "                    _logs.append(_log)\n",
    "                \n",
    "                if showMessages:\n",
    "                    pp.progressbar(100, message_text=f'Attempt {_attempts}: Successfully saved all tables')\n",
    "            else:\n",
    "                raise ValueError(f'Error: {_message}')\n",
    "            etl_refresh_node.node.invalidate()\n",
    "            \n",
    "            # Calculate elapsed time of execution\n",
    "            _elapsed_time = _time_mod.time() - _start_time\n",
    "            _minutes = _elapsed_time/60\n",
    "            _minutes_round = int(_minutes)\n",
    "            _seconds = int((_minutes-_minutes_round) * 60)\n",
    "            if showMessages:\n",
    "                pp.send_message(message_text=f'{_minutes_round}:{_seconds:02d} minutes', message_title='Elapsed time')\n",
    "        else:\n",
    "            _recurrent_tables = recurrentTables\n",
    "            _recurrent_tables_str = ', '.join(_recurrent_tables)\n",
    "            _log = create_log_row(processName, _username, '', '', 'RECURRENT TABLES ERROR', f'Tables {_recurrent_tables_str} had different versions')\n",
    "            _logs.append(_log)\n",
    "    \n",
    "    except Exception as e:\n",
    "        _log = create_log_row(processName, _username, '', _failed_table, 'UNEXPECTED ERROR', f'Error: {e}')\n",
    "        _logs.append(_log)\n",
    "    \n",
    "    finally:\n",
    "        # Join logs and save them\n",
    "        save_logs(_logs, logsPath=logsPath)\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Saved Version\n",
    "\n",
    "def _fn(versionsPath, versionName, processName):\n",
    "    import os\n",
    "    import shutil\n",
    "    import datetime\n",
    "    import pytz\n",
    "    \n",
    "    _version = versionName\n",
    "    _version_path = os.path.join(versionsPath, _version)\n",
    "    _username = pyplan_user['userName']\n",
    "    \n",
    "    if os.path.exists(_version_path):\n",
    "        shutil.rmtree(_version_path, ignore_errors=True)\n",
    "        \n",
    "        # Update logs\n",
    "        _log = create_log_row(\n",
    "            processName=processName,\n",
    "            userName=_username,\n",
    "            version=versionName,\n",
    "            tableName='[ALL]',\n",
    "            status='OK',\n",
    "            message='')\n",
    "    \n",
    "        # Join logs and save again\n",
    "        if os.path.isfile(etl_logs_file_path):\n",
    "            _df_all_logs = pd.read_pickle(etl_logs_file_path, compression=etl_pkl_compression_mode)\n",
    "            _df_all_logs = pd.concat([_df_all_logs, _log], ignore_index=True)[_log.columns]\n",
    "        else:\n",
    "            _df_all_logs = _log.copy()\n",
    "        _df_all_logs.to_pickle(etl_logs_file_path, compression=etl_pkl_compression_mode)\n",
    "        \n",
    "        pp.send_message(f\"Succesfully removed '{_version}' version\")\n",
    "        etl_refresh_node.node.invalidate()\n",
    "    else:\n",
    "        pp.send_message(f\"Version '{_version}' does not exist\")\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Log Row\n",
    "\n",
    "def _fn(processName, userName, version, tableName, status, message):\n",
    "    import datetime\n",
    "    import pytz\n",
    "    \n",
    "    _df = pd.DataFrame({\n",
    "        'Process': processName,\n",
    "        'Username': userName,\n",
    "        'Version': version,\n",
    "        'Table': tableName,\n",
    "        'Timestamp': datetime.datetime.now(pytz.timezone(etl_pytz_timezone)).strftime('%Y/%m/%d %H:%M:%S'),\n",
    "        'Status': status,\n",
    "        'Message': message}, index=[0])\n",
    "    \n",
    "    return _df\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Logs\n",
    "\n",
    "def _fn(logs, logsPath):\n",
    "    import os\n",
    "    \n",
    "    if len(logs) > 0:\n",
    "        _df_logs = pd.concat(logs, ignore_index=True)\n",
    "        if os.path.isfile(logsPath):\n",
    "            _df_all_logs = pd.read_pickle(logsPath, compression=etl_pkl_compression_mode)\n",
    "            _df_all_logs = pd.concat([_df_all_logs, _df_logs], ignore_index=True)[_df_logs.columns]\n",
    "        else:\n",
    "            _df_all_logs = _df_logs.copy()\n",
    "        _df_all_logs.to_pickle(logsPath, compression=etl_pkl_compression_mode)\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload File to Datalake\n",
    "\n",
    "def _fn(fileSystemClient, filesPath, localFilePath, targetFilename):\n",
    "    \n",
    "    _directory_client = fileSystemClient.get_directory_client(filesPath)\n",
    "    _file_client = _directory_client.create_file(targetFilename)\n",
    "    \n",
    "    with open(localFilePath, 'rb') as local_file:\n",
    "        _file_contents = local_file.read()\n",
    "        _file_client.append_data(data=_file_contents, offset=0, length=len(_file_contents))\n",
    "        _file_client.flush_data(len(_file_contents))\n",
    "        \n",
    "    return 'OK'\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Files From Exports Folder\n",
    "\n",
    "def _fn(daysElapsed, showMessages=True):\n",
    "    import os\n",
    "    import time as _time_mod\n",
    "    \n",
    "    _current_time = _time_mod.time()\n",
    "    _minimum_time = _current_time - (daysElapsed*24*60*60)\n",
    "    _count = 0\n",
    "    for _, _, filenames in os.walk(etl_exports_to_dl_folder_path):\n",
    "        for filename in filenames:\n",
    "            _filepath = os.path.join(etl_exports_to_dl_folder_path, filename)\n",
    "            _file_mtime = os.path.getmtime(_filepath)\n",
    "            if _file_mtime < _minimum_time:\n",
    "                os.remove(_filepath)\n",
    "                _count += 1\n",
    "    \n",
    "    if showMessages:\n",
    "        _message = f'Removed {_count} files from Exports folder' if _count > 0 else 'No files were removed from Exports folder'\n",
    "        pp.send_message(_message)\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Records From Table\n",
    "\n",
    "def _fn(mainTable, path, connection, cursor, elapsedDays=10, showMessages=True):\n",
    "    \n",
    "    import sqlite3\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    _threshold_date = opt_current_date - timedelta(days=elapsedDays)\n",
    "    _threshold_date_str = _threshold_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Remove old rows from original table\n",
    "    _delete_query = f\"\"\"\n",
    "    DELETE FROM {mainTable}\n",
    "    WHERE lastUpdate < '{_threshold_date_str}'\n",
    "    \"\"\"\n",
    "    cursor.execute(_delete_query)\n",
    "    \n",
    "    if showMessages:\n",
    "        pp.send_message(f'Succesfully deleted records from {mainTable} older than {_threshold_date_str}')\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Old Records From Forms Tables\n",
    "\n",
    "def _fn(tables, pathNode, elapsedDays=10, showMessages=True):\n",
    "    \n",
    "    import sqlite3\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    _conn = sqlite3.connect(pathNode.result)\n",
    "    _cursor = _conn.cursor()\n",
    "    \n",
    "    for table in tables:\n",
    "        remove_old_records_from_table( \n",
    "            mainTable=table,\n",
    "            path=pathNode.result,\n",
    "            connection=_conn,\n",
    "            cursor=_cursor,\n",
    "            elapsedDays=elapsedDays,\n",
    "            showMessages=showMessages)\n",
    "    \n",
    "    # Rebuild database (shrinks file size)\n",
    "    _vacuum_query = 'VACUUM'\n",
    "    _cursor.execute(_vacuum_query)\n",
    "    \n",
    "    _conn.commit()\n",
    "    _conn.close()\n",
    "    \n",
    "    pathNode.invalidate()\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive Old Records From Table\n",
    "\n",
    "def _fn(mainTable, archiveTable, path, connection, cursor, elapsedDays=10, showMessages=True):\n",
    "    \n",
    "    import sqlite3\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    _threshold_date = opt_current_date - timedelta(days=elapsedDays)\n",
    "    _threshold_date_str = _threshold_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Get columns for insert statement\n",
    "    _cols_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {mainTable}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    _cols = pd.read_sql_query(_cols_query, connection).columns.tolist()\n",
    "    if 'id' in _cols:\n",
    "        _cols.remove('id')\n",
    "    _cols_str = ','.join([f'[{col}]' for col in _cols])\n",
    "    \n",
    "    # Insert old rows into archive table\n",
    "    _insert_query = f\"\"\"\n",
    "    INSERT INTO {archiveTable} ({_cols_str})\n",
    "    SELECT {_cols_str}\n",
    "    FROM {mainTable}\n",
    "    WHERE lastUpdate < '{_threshold_date_str}'\n",
    "    \"\"\"\n",
    "    cursor.execute(_insert_query)\n",
    "    \n",
    "    # Remove old rows from original table\n",
    "    _delete_query = f\"\"\"\n",
    "    DELETE FROM {mainTable}\n",
    "    WHERE lastUpdate < '{_threshold_date_str}'\n",
    "    \"\"\"\n",
    "    cursor.execute(_delete_query)\n",
    "    \n",
    "    if showMessages:\n",
    "        pp.send_message(f'Succesfully archived records from {mainTable} older than {_threshold_date_str}')\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive Old Records From Forms Tables\n",
    "\n",
    "def _fn(tablePairs, pathNode, elapsedDays=10, showMessages=True):\n",
    "    \n",
    "    import sqlite3\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    _conn = sqlite3.connect(pathNode.result)\n",
    "    _cursor = _conn.cursor()\n",
    "    \n",
    "    for pair in tablePairs:\n",
    "        archive_old_records_from_table( \n",
    "            mainTable=pair[0],\n",
    "            archiveTable=pair[1],\n",
    "            path=pathNode.result,\n",
    "            connection=_conn,\n",
    "            cursor=_cursor,\n",
    "            elapsedDays=elapsedDays,\n",
    "            showMessages=showMessages)\n",
    "    \n",
    "    # Rebuild database (shrinks file size)\n",
    "    _vacuum_query = 'VACUUM'\n",
    "    _cursor.execute(_vacuum_query)\n",
    "    \n",
    "    _conn.commit()\n",
    "    _conn.close()\n",
    "    \n",
    "    pathNode.invalidate()\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XML Message\n",
    "\n",
    "def _fn(message):\n",
    "    \n",
    "    _xml = f\"\"\"<?xml version='1.0' encoding='utf-8'?>\n",
    "    <QueueMessage>  \n",
    "        <MessageText>{message}</MessageText>  \n",
    "    </QueueMessage>\"\"\"\n",
    "    \n",
    "    return _xml\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST Message to Datalake\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def _fn(message, url):\n",
    "    _retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        method_whitelist=False\n",
    "    )\n",
    "    _adapter = HTTPAdapter(max_retries=_retry_strategy)\n",
    "    _http = requests.Session()\n",
    "    _http.mount('https://', _adapter)\n",
    "    _http.mount('http://', _adapter)\n",
    "    _data_xml = create_xml_message(message)\n",
    "    _req = _http.post(url, data=_data_xml, timeout=5)\n",
    "    pp.send_message(_req.status_code)\n",
    "    return _req\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST Message to Datalake Old\n",
    "\n",
    "def _fn(message, url):\n",
    "    import requests\n",
    "\n",
    "    _data_xml = create_xml_message(message)\n",
    "    _req = requests.post(url, data=_data_xml)\n",
    "    \n",
    "    return _req\n",
    "\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Time\n",
    "\n",
    "_try_convert = try_convert\n",
    "\n",
    "def _fn(timeSerie):\n",
    "    timeSerie = timeSerie.astype(str).apply(lambda x: (x[:4] + '.' + (x[5:] + '0' if len(x)<7 else x[5:])) if ( isinstance(_try_convert(x[:4],x,int),int) & (_try_convert(x,x,float)-_try_convert(_try_convert(x,x,float),x,int) != 0 )) else int(x[:4]) if ( isinstance(_try_convert(x[:4],x,int),int) & (_try_convert(x,x,float) - _try_convert(_try_convert(x,x,float),x,int) == 0))\n",
    "    else x).astype(str)\n",
    "    \n",
    "    return timeSerie\n",
    "result = _fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Convert\n",
    "\n",
    "def _try_convert(value, default, *types):\n",
    "    for t in types:\n",
    "        try:\n",
    "            return t(value)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return default\n",
    "result = _try_convert"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75b64ca35833826d255b5110be976b8b491e59919d991eceebad02336ca41c97"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
